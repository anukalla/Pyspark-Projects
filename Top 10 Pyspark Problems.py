# Databricks notebook source
#anagram python

s1 = "silent"
s2 = "listen"
def check(s1, s2):
    if(sorted(s1)==sorted(s2)):
        print("the strings are anagrams.")
    else:
        print("the strings aren't anagram.")
s1 = "silent"
s2 = "listen"
check(s1, s2)

# COMMAND ----------

#dbfs:/FileStore/shared_uploads/anusha.kalla1996@gmail.com/transaction_data.csv
from pyspark.sql import *
from pyspark.sql.types import *
from pyspark.sql.functions import *
 
spark = SparkSession.builder.appName("Read").getOrCreate()


# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.types import *
 
# Define the schema
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType

my_schema = StructType([
    StructField("transaction_id", IntegerType(), True),
    StructField("customer_id", IntegerType(), True),
    StructField("product", StringType(), True),
    StructField("quantity", IntegerType(), True),
    StructField("price", DoubleType(), True),
    StructField("date", StringType(), True),
    StructField("region", StringType(), True)
])
 
# Read the CSV file using the defined schema
df = spark.read.format("csv") \
    .schema(my_schema) \
    .option("header", True) \
    .load('/FileStore/shared_uploads/anusha.kalla1996@gmail.com/transaction_data.csv')
 
df.display()
df.printSchema()
 

# COMMAND ----------

# read the csv file

df_final = spark.read.format("csv").option("interschema",True).option("header",True).load('/FileStore/shared_uploads/anusha.kalla1996@gmail.com/transaction_data.csv')
df_final.display()

# COMMAND ----------

#1. Find the total sales amount for each region.
from pyspark.sql import functions as F
df_final_1 = df.withColumn("sales_amount", F.col("quantity") * F.col("price"))
df_final_1.display()



# COMMAND ----------

#2. Identify the most purchased product by each customer.
df_total_sales_by_region = df_final_1.groupBy("region").agg(F.sum("sales_amount").alias("total_sales"))
df_total_sales_by_region.display()

df_aggregated = df_final_1.groupBy("customer_id", "product").agg(
    sum("quantity").alias("total_quantity")
)

df_aggregated.display()

df_top_product = df_aggregated.groupBy("customer_id").agg(max(struct(col("total_quantity"), col("product"))).alias("top_product"))

df_top_product.display()
 
df_top_product = df_top_product.select(col("customer_id"),col("top_product.product").alias("most_purchased_product"),col("top_product.total_quantity").alias("total_quantity"))
 
df_top_product.display()

# COMMAND ----------

#Calculate the total revenue generated by each product.
from pyspark.sql.window import *

window = Window.partitionBy("customer_id").orderBy(col("total_quantity").desc())
df_window = df_aggregated.withColumn("rn",row_number().over(window))
df_window.display()


# COMMAND ----------

from pyspark.sql.functions import col

df_ = df_window.select("customer_id", "product", "total_quantity").filter(col("rn").cast("int") == 1)
df_.display()

# COMMAND ----------

#Calculate the total revenue generated by each product.

df_final_1.display()

# COMMAND ----------

df_ques3 = df_final_1.groupBy("product").agg(sum("sales_amount").alias("total_sales_by_region"))
df_ques3.display()

# COMMAND ----------

#. Find the region with the highest total sales.

df_ques4 = df_final_1.groupBy("region").agg(sum("sales_amount").alias("total_sales_by_region"))\
    .orderBy(col("total_sales_by_region").desc())
df_ques4.limit(1).display()

# COMMAND ----------

#Find customers who purchased more than or equal to 10 units of any product.

df_final_1.filter(col("quantity")>=10).display()

# COMMAND ----------

#__Calculate average price per product across all regions.
df_final_1.groupBy("product").agg(avg("price")).display()

# COMMAND ----------

#Count the number of distinct customers in each region
df_final_1.display()

# COMMAND ----------

df_final_1.groupBy("region").agg(countDistinct("customer_id")).display()
